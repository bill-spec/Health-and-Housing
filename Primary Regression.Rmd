---
title: "Primary Regression"
author: "Bill Lang"
date: "1/25/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet)
library(car)       #vif
library(rpart)     #Tree
library(partykit) #Special Tree Stuff
library(MASS)
library(caret)
library(leaps)
```

Reading in the data.

```{r}
dataSet
```


```{r}
ggplot(gather(data = dataSet), aes(value)) + 
    geom_histogram(bins = 10) + 
    facet_wrap(~key, scales = 'free_x')
```

```{r}
pairs(rentTotalPop ~ TotalPercentEBLL15_18 + anxietyPorportion + depressionPorportion + hyper2Porportion + Dia2Porportion + TotalPopulation + ResidentalBuildings + percent.of.vacant.properties, dataSet)
```


```{r}
renterModel <- lm(rentTotalPop~ TotalPercentEBLL15_18 + anxietyPorportion + depressionPorportion + hyper2Porportion + Dia2Porportion +  percent.of.vacant.properties,data = dataSet)

summary(renterModel)

anova(renterModel)
```


```{r}
dataSet$rentTotalPop
```


```{r}
vif(renterModel)
```

Collinearity looks safe. 

```{r}
par(mfrow = c(2,2))
plot(renterModel)
```

For the sake of dealing with outliers we will remove the $20100$ census tract observation. This observation was signifigantly impacting the model. 

```{r}
datasetResidual <- dataSet[!row.names(dataset) %in% 20100,]
datasetResidual
```


```{r}
renterModelResidual <- lm(rentTotalPop~ TotalPercentEBLL15_18 + anxietyPorportion + depressionPorportion + hyper2Porportion + Dia2Porportion + percent.of.vacant.properties + TotalPercentEBLL15_18:anxietyPorportion + TotalPercentEBLL15_18:depressionPorportion + TotalPercentEBLL15_18:hyper2Porportion + TotalPercentEBLL15_18:Dia2Porportion,data = datasetResidual)

summary(renterModelResidual)

anova(renterModelResidual)
```


```{r}
vif(renterModelResidual)
```

Collinearity looks safe. 

```{r}
par(mfrow = c(2,2))
plot(renterModelResidual)
```

```{r}
names(renterModelResidual)
```

Best Subset Selection. Looking at the plots we can see that we should probably remove those variables that attempt to find interaction between diabetes and hypertension. The average lead amoung for the region has also been removed in all the variable selective models. It will be left in for the time being for heiracrchical organziation as it would remain as an interaction term between anxiety and depression anyways. 

**Can be imporved by only using the training datasets to select as this will be have high varience**

```{r}
?regsubsets
regfull <- regsubsets(rentTotalPop~ TotalPercentEBLL15_18 + anxietyPorportion + depressionPorportion + hyper2Porportion + Dia2Porportion  + percent.of.vacant.properties + TotalPercentEBLL15_18:anxietyPorportion + TotalPercentEBLL15_18:depressionPorportion + TotalPercentEBLL15_18:hyper2Porportion + TotalPercentEBLL15_18:Dia2Porportion,data = datasetResidual,nvmax = 13)
regsummary <-summary(regfull)

names(regsummary)


plot(regfull, scale = "r2")
plot(regfull, scale = "adjr2")
plot(regfull, scale = "Cp")
plot(regfull, scale = "bic")

```

Rebuilding the model again after the subset selection process. THis model is approaching what would be considered a final model. 

```{r}
renterModelAfterSub <- lm(rentTotalPop~ TotalPercentEBLL15_18 + anxietyPorportion + depressionPorportion + hyper2Porportion + Dia2Porportion +   percent.of.vacant.properties + TotalPercentEBLL15_18:anxietyPorportion + TotalPercentEBLL15_18:depressionPorportion,data = datasetResidual)

summary(renterModelAfterSub)

anova(renterModelAfterSub)
```

A partial F-test between makes it clear that yes, those two interactions were not signifigant. The Cumulative distribution of our F-values (0.8898) lets us know there is no signifigance between the two. 

```{r}
anova(renterModelAfterSub,renterModelResidual)
```

Collinearity is much higher between our interaction terms than before but if that is okay it looks safe to use. 

```{r}
vif(renterModelAfterSub)
```

```{r}
par(mfrow = c(2,2))
plot(renterModelAfterSub)
```

```{r}
names(renterModelAfterSub)
```























Checking for overfitting. There is little reason to use a model like this for prediction; however, we can still go through the motions of contructing a good model to prvent overfitting. 

```{r}
trainid <- sample(1:nrow(datasetResidual), nrow(datasetResidual)*0.8 , replace=F)
train <- datasetResidual[trainid,]
test <- datasetResidual[-trainid,]
dim(train); dim(test)
```

```{r}
renterModelTrain <- lm(rentTotalPop~ TotalPercentEBLL15_18 + anxietyPorportion + depressionPorportion + hyper2Porportion + Dia2Porportion + percent.of.vacant.properties + TotalPercentEBLL15_18:anxietyPorportion + TotalPercentEBLL15_18:depressionPorportion,data = train)

MSE0 <- mean( (predict(renterModelTrain, test) - test$rentTotalPop)^2 )
MSE0
```

Cross Validation using the caret package. The difference between our Cross Validated Error and our origional simple train-test set is only $0.016$. We can say that it is very likely the true error rate of the model is close to $0.03$ or for a root squared error $0.12$ which is about $1/2$ the standard deviation of the rentTotalPop  variable ($0.21$). 

```{r}
train.control <- trainControl(method = "cv", number = 10)

model <- train(rentTotalPop ~., data = datasetResidual, method = "lm",
               trControl = train.control)

print(model)

CVMSE <- (model$results[2])^2
as.double(abs(MSE0 - CVMSE))
```

```{r}
sd(datasetResidual$rentTotalPop)
```





















Plotting to look for non-linearity

```{r}
pairsData <- regSmall %>% dplyr::select(rentTotalPop, percent.of.vacant.properties,  TotalPercentEBLL15_18, depressionPorportion, TotalPercentEBLL15_18,hyperPorportion, anxietyPorportion,TotalPopulation,DiaPorportion)

pairs(pairsData)
```

Rent per total population testing model. 

```{r}
smallNormal <- lm(rentTotalPop ~  percent.of.vacant.properties + I(percent.of.vacant.properties^2), regSmall)

smallPoly <- lm(rentTotalPop ~  poly(percent.of.vacant.properties,2), regSmall)

summary(smallNormal)
summary(smallPoly)
```











Rent per total population model

```{r}

smallAll <- lm(rentTotalPop ~  percent.of.vacant.properties + I(percent.of.vacant.properties^2) + TotalPercentEBLL15_18 + I(TotalPercentEBLL15_18^2) +  depressionPorportion + TotalPercentEBLL15_18 + log(hyperPorportion) + anxietyPorportion + TotalPopulation + DiaPorportion + anxietyPorportion*percent.of.vacant.properties, regSmall)

smallAll <- lm(rentTotalPop ~  poly(percent.of.vacant.properties,2)+ TotalPercentEBLL15_18 + I(TotalPercentEBLL15_18^2) +  depressionPorportion + TotalPercentEBLL15_18 + log(hyperPorportion) + anxietyPorportion + TotalPopulation + DiaPorportion, regSmall)

summary(smallAll)

smallAll2 <- lm(rentTotalPop ~  TotalPercentEBLL15_18 + I(TotalPercentEBLL15_18^2) +  depressionPorportion + TotalPercentEBLL15_18 + log(hyperPorportion) + anxietyPorportion + DiaPorportion , regSmall)

summary(smallAll)

plot(smallAll)

anova(smallAll)

vif(smallAll)
```

Percent Vacant

```{r}
smallAgain <- lm(percent.of.vacant.properties ~ rentTotalPop  +  I(rentTotalPop^2) + TotalPercentEBLL15_18 + I(TotalPercentEBLL15_18^2) + depressionPorportion + TotalPercentEBLL15_18 + hyperPorportion + anxietyPorportion + I(anxietyPorportion^2) + TotalPopulation + DiaPorportion, regSmall)

summary(smallAgain)

summary(stepAIC(smallAgain))

plot(smallAgain)

vif(smallAgain)
```





Logistic Model 

```{r}
hist(regSmall$rentTotalPop,300)
```

```{r}
class <- regSmall %>% 
  mutate(rent = ifelse((rentTotalPop >= 0.4), 1, 0))
head(class)

trainid <- sample(1:nrow(class), nrow(regSmall)*0.8 , replace=F)
train <- class[trainid,]
test <- class[-trainid,]
dim(train); dim(test)


logit <- glm(rent ~ percent.of.vacant.properties  + I(percent.of.vacant.properties^2) +  TotalPercentEBLL15_18 + I(TotalPercentEBLL15_18^2)+  depressionTotalPop + TotalPercentEBLL15_18 + hyperTotalPop + anxietyTotalPop + TotalPopulation + DiaTotalPop, class, family = "binomial")

logitProb <- predict(logit, test, type = "response")

logitPred <- ifelse(logitProb > 0.5, 1, 0)

table(logitPred, test$rent)

mean(logitPred != test$rent)
```




Checking for overfitting 

```{r}
trainid <- sample(1:nrow(regSmall), nrow(regSmall)*0.8 , replace=F)
train <- regSmall[trainid,]
test <- regSmall[-trainid,]
dim(train); dim(test)
```

```{r}
modelTrain <- lm(rentTotalPop ~ percent.of.vacant.properties  + I(percent.of.vacant.properties^2) +  TotalPercentEBLL15_18 + I(TotalPercentEBLL15_18^2) +  depressionTotalPop + TotalPercentEBLL15_18 + log(hyperTotalPop) + anxietyTotalPop + TotalPopulation + DiaTotalPop, train)

MSE0 <- mean( (predict(modelTrain, test) - test$rentTotalPop)^2 )
MSE0
```

Cross Validation using the caret package

```{r}
train.control <- trainControl(method = "cv", number = 10)

model <- train(rentTotalPop ~., data = pairsData, method = "lm",
               trControl = train.control)

print(model)
```

